{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleAtlanta GPS Cleaning\n",
    "This notebook is for cleaning the GPS traces from CycleAtlanta. It's based on the\n",
    "GPS_Clean_Route_Snapping.sql script that Dr. Aditi Misra used.\n",
    "\n",
    "Overview\n",
    "1. Imports and combines all of the coords.csv files (in the future we should just work from the original sql)\n",
    "1. Add column names and format data\n",
    "1. Remove trips with fewer than 5 minutes of data recorded (modifiable)\n",
    "1. Create a trip dataframe to cross reference with trip.csv (only keep trips that are in both)\n",
    "1. Drop trip duplicates\n",
    "1. Turn coords df into dictionary\n",
    "1. Remove points with unrealisic jumps (> 47mph)\n",
    "1. With the unrealistic ones removed, only consider trip inside the specified area (inside the i-285 perimeter for this study)\n",
    "1. Split trips that have a pause of 5 mins or more\n",
    "1. Export the coords and trips into a pkl file (replace with a .db in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import gps_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load((Path.cwd() / 'config.json').open('rb'))\n",
    "export_fp = Path(config['project_directory']) / 'CycleAtlanta'\n",
    "if export_fp.exists() == False:\n",
    "    export_fp.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepaths for traces\n",
    "cycleatl_fp = Path.home() / 'Documents/BikewaySimData/Data/CycleAtlanta'\n",
    "coords_fps = cycleatl_fp.glob('coord*.csv')\n",
    "\n",
    "#coordinate reference system to project to\n",
    "project_crs = config['projected_crs_epsg']\n",
    "\n",
    "#set the minimum number of points required for a trip to be considered\n",
    "#300 is based on the number of points needed for a second-by-second trace of five minutes\n",
    "point_threshold = 5 * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in raw gps coordinate files from CycleAtlanta\n",
    "There are several coordinate files, and the some trips span across them so this\n",
    "block loads them all into memory (~5 mins for computer with 32GB RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading coord-1.csv\n",
      "Reading coord-2.csv\n",
      "Reading coord-3.csv\n",
      "Reading coord-4.csv\n",
      "Reading coord-5.csv\n",
      "Reading coord.csv\n",
      "58631361 total points found\n",
      "28240 initial trips found\n"
     ]
    }
   ],
   "source": [
    "# initialize empty dataframe and dictionary for adding cleaned data\n",
    "coords = pd.DataFrame()\n",
    "\n",
    "#add each coords csv to all coords dataframe (only possible if lots of RAM)\n",
    "#TODO modify to run this step with SQL\n",
    "# NOTE some trips span across the different CSV files so retain last trip ids somewhere if you have to read\n",
    "# in one at a time\n",
    "for coords_fp in coords_fps:\n",
    "    print('Reading',coords_fp.name)\n",
    "    one_coords = pd.read_csv(coords_fp,header=None)\n",
    "    coords = pd.concat([coords,one_coords],ignore_index=True)\n",
    "\n",
    "# rename columns\n",
    "col_names = ['tripid','datetime','lat','lon','altitude_m','speed_kph','hAccuracy_m','vAccuracy_m']\n",
    "coords.columns = col_names\n",
    "\n",
    "# replace -1 speed with NA\n",
    "coords.loc[coords['speed_kph']==-1,'speed_kph'] = np.nan\n",
    "\n",
    "# convert speed and accuracy to imperial units (mph and ft)\n",
    "coords['speed_mph'] = coords['speed_kph'] * 2.2369362920544025\n",
    "coords['hAccuracy_ft'] = coords['hAccuracy_m'] * 3.28084 \n",
    "\n",
    "# drop unneeded ones\n",
    "coords.drop(columns=['altitude_m','hAccuracy_m','vAccuracy_m','speed_kph'],inplace=True)\n",
    "\n",
    "# change dates to datetime\n",
    "coords['datetime'] = pd.to_datetime(coords['datetime'])\n",
    "\n",
    "# add geometry info and turn into geodataframe\n",
    "coords['geometry'] = gpd.points_from_xy(coords['lon'],coords['lat'])\n",
    "coords = gpd.GeoDataFrame(coords,geometry='geometry',crs='epsg:4326')\n",
    "coords.to_crs(project_crs,inplace=True)\n",
    "\n",
    "#add X/Y column\n",
    "coords['X'] = coords.geometry.x\n",
    "coords['Y'] = coords.geometry.y\n",
    "\n",
    "# sort everything\n",
    "coords.sort_values(['tripid','datetime'],inplace=True)\n",
    "\n",
    "# count and print number of trips\n",
    "print(f\"{coords.shape[0]} total points found\")\n",
    "print(f\"{coords['tripid'].nunique()} initial trips found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 10000000 duplicate points\n",
      "48631361 total points remaining\n"
     ]
    }
   ],
   "source": [
    "dups = coords[['tripid','datetime']].duplicated()\n",
    "coords = coords[~dups]\n",
    "print('Dropped',dups.sum(),'duplicate points')\n",
    "print(f\"{coords.shape[0]} total points remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove trips fewer than 300 points (theoretical max for 5 minute trip with at least 1 sec sampling)\n",
    "Doing it this way because duration alone does not say anything about the amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2737 trips had fewer than 300 points 25503 trips remaining\n"
     ]
    }
   ],
   "source": [
    "below_5 = (coords['tripid'].value_counts() < point_threshold)\n",
    "coords = coords[~coords['tripid'].isin(below_5[below_5].index.tolist())]\n",
    "print(below_5.sum(),f'trips had fewer than {point_threshold} points',coords['tripid'].nunique(),'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841 trips were partially outside of the study area\n",
      "2586 trips were completey outside of the study area\n",
      "22076 trips remaining\n"
     ]
    }
   ],
   "source": [
    "studyarea = gpd.read_file(Path(config['studyarea'])).unary_union\n",
    "within = coords.clip(studyarea)\n",
    "within = pd.merge(coords['tripid'].value_counts(),within['tripid'].value_counts(),left_index=True,right_index=True,how='left')\n",
    "\n",
    "### Completely within study area\n",
    "completely_within = within[within['tripid_x'] == within['tripid_y']].index.tolist()\n",
    "\n",
    "### Partially within study area\n",
    "partial = within[within['tripid_x'] > within['tripid_y']].index.tolist()\n",
    "print(len(partial),'trips were partially outside of the study area')\n",
    "\n",
    "### Completely out of study area\n",
    "no_overlap = within[within['tripid_y'].isna()].index.tolist()\n",
    "print(len(no_overlap),'trips were completey outside of the study area')\n",
    "\n",
    "coords = coords[coords['tripid'].isin(completely_within)]\n",
    "print(coords['tripid'].nunique(),'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create trips dataframe\n",
    "These next lines aggregate the points by trip id to find the start/end time/location,\n",
    "duration, numbner of points, and average hAccuracy. These are recorded as initial values to compare against the cleaned dataset. Then find and remove duplicate trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get start time\n",
    "start_time = coords.groupby('tripid')['datetime'].min()\n",
    "start_time.rename('initial_start_time',inplace=True)\n",
    "\n",
    "#get end time\n",
    "end_time = coords.groupby('tripid')['datetime'].max()\n",
    "end_time.rename('initial_end_time',inplace=True)\n",
    "\n",
    "#get duration\n",
    "duration = end_time - start_time\n",
    "duration.rename('initial_duration',inplace=True)\n",
    "\n",
    "#get starting location unprojected\n",
    "start_lon = coords.groupby('tripid')['datetime'].idxmin().map(coords['lon'])\n",
    "start_lat = coords.groupby('tripid')['datetime'].idxmin().map(coords['lat'])\n",
    "start_lon.rename('initial_start_lon',inplace=True)\n",
    "start_lat.rename('initial_start_lat',inplace=True)\n",
    "\n",
    "#get ending location unprojected\n",
    "end_lon = coords.groupby('tripid')['datetime'].idxmax().map(coords['lon'])\n",
    "end_lat = coords.groupby('tripid')['datetime'].idxmax().map(coords['lat'])\n",
    "end_lon.rename('initial_end_lon',inplace=True)\n",
    "end_lat.rename('initial_end_lat',inplace=True)\n",
    "\n",
    "#get starting location projected\n",
    "start_X = coords.groupby('tripid')['datetime'].idxmin().map(coords['X'])\n",
    "start_Y = coords.groupby('tripid')['datetime'].idxmin().map(coords['Y'])\n",
    "start_X.rename('initial_start_X',inplace=True)\n",
    "start_Y.rename('initial_start_Y',inplace=True)\n",
    "\n",
    "#get ending location projected\n",
    "end_X = coords.groupby('tripid')['datetime'].idxmax().map(coords['X'])\n",
    "end_Y = coords.groupby('tripid')['datetime'].idxmax().map(coords['Y'])\n",
    "end_X.rename('initial_end_X',inplace=True)\n",
    "end_Y.rename('initial_end_Y',inplace=True)\n",
    "\n",
    "#get number of points\n",
    "num_of_points = coords['tripid'].value_counts()\n",
    "num_of_points.rename('initial_total_points',inplace=True)\n",
    "\n",
    "#get average haccuracy\n",
    "avg_accuracy = coords.groupby('tripid')['hAccuracy_ft'].mean()\n",
    "avg_accuracy.rename('initial_avg_accuracy',inplace=True)\n",
    "\n",
    "#turn into df\n",
    "trips_df = pd.concat([start_time,end_time,\n",
    "                      start_lon,start_lat,end_lon,end_lat,\n",
    "                      start_X,start_Y,end_X,end_Y,\n",
    "                      duration,num_of_points,avg_accuracy],axis=1)\n",
    "trips_df.reset_index(inplace=True)\n",
    "trips_df.rename(columns={'index':'tripid'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross reference with the trip table\n",
    "It appears that there are more trips in the coords data than in the trip csv. However, these are all past 2016, so we'll remove them for now.\n",
    "\n",
    "Also, the start times in trip.csv do not line up with the coords.csv, but the end times and initial_num_points do. Used start times from coords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 trips are not in trip.csv and have been dropped\n",
      "22067 trips remaining\n"
     ]
    }
   ],
   "source": [
    "trip = pd.read_csv(cycleatl_fp/'trip.csv',header=None)\n",
    "col_names = ['tripid','userid','trip_type','description','starttime','endtime','initial_num_points']\n",
    "trip.columns = col_names\n",
    "trip.drop(columns=['starttime','endtime','initial_num_points'],inplace=True)\n",
    "before_merge = trips_df.shape[0]\n",
    "trips_df = pd.merge(trips_df,trip,on='tripid')\n",
    "print(before_merge-trips_df.shape[0],'trips are not in trip.csv and have been dropped')\n",
    "\n",
    "#remove tripids from coords_df\n",
    "coords = coords[coords['tripid'].isin(set(trips_df['tripid'].tolist()))]\n",
    "print(trips_df.shape[0],'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484 trips are duplicates\n"
     ]
    }
   ],
   "source": [
    "duplicates = trips_df.drop(columns=['tripid']).duplicated()\n",
    "print(f\"{duplicates.sum()} trips are duplicates\")\n",
    "duplicate_tripids = trips_df[duplicates]['tripid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20583 trips remaining\n"
     ]
    }
   ],
   "source": [
    "trips_df = trips_df[~duplicates]\n",
    "coords = coords[~coords['tripid'].isin(duplicate_tripids)]\n",
    "print(trips_df.shape[0],'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove points after pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717 trips have at least one 2 minute pause\n"
     ]
    }
   ],
   "source": [
    "pause_threshold_min = 2\n",
    "coords['time_diff'] = coords.groupby('tripid')['datetime'].apply(lambda x: x.diff())\n",
    "trips_above_threshold = coords.loc[coords['time_diff'] > datetime.timedelta(minutes=pause_threshold_min),'tripid'].unique().tolist()\n",
    "print(len(trips_above_threshold),'trips have at least one',pause_threshold_min,'minute pause')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_points_before_pause(group):\n",
    "    \n",
    "    group = group.copy()\n",
    "\n",
    "    #get indices of all pauses\n",
    "    idxs = group[group['time_diff'] > datetime.timedelta(minutes=pause_threshold_min)].index.tolist()\n",
    "\n",
    "    #go through each index\n",
    "    for idx in idxs:\n",
    "        #retrieve the time stamp\n",
    "        pause_time = group.loc[idx,'datetime']\n",
    "        \n",
    "        #count before and after\n",
    "        before = group['datetime'] < pause_time\n",
    "        after = group['datetime'] >= pause_time\n",
    "\n",
    "        # if before.sum() >= after.sum():\n",
    "        #     return group[before]\n",
    "        if before.sum() >= point_threshold:\n",
    "            return group[before]\n",
    "        else:\n",
    "            group = group[after]\n",
    "    \n",
    "    #if the last group is the only one choose that\n",
    "    return group\n",
    "\n",
    "coords = coords.groupby('tripid').apply(keep_points_before_pause).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 trips have at least one 2 minute pause\n"
     ]
    }
   ],
   "source": [
    "pause_threshold_min = 2\n",
    "coords['time_diff'] = coords.groupby('tripid')['datetime'].apply(lambda x: x.diff())\n",
    "trips_above_threshold = coords.loc[coords['time_diff'] > datetime.timedelta(minutes=pause_threshold_min),'tripid'].unique().tolist()\n",
    "print(len(trips_above_threshold),'trips have at least one',pause_threshold_min,'minute pause')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Filter\n",
    "Sustained speeds over 20 mph hint that the trip is probably a car trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 trips have sustained speeds above 30 mph\n",
      "20430 trips remaining\n"
     ]
    }
   ],
   "source": [
    "maxspeed = 30\n",
    "high_maxspeed = 100\n",
    "sustained_speeds = coords.groupby('tripid')['speed_mph'].apply(lambda speed: (((speed > maxspeed) & (speed < high_maxspeed)).sum() > 1*60))\n",
    "sustained_speeds = sustained_speeds[sustained_speeds].index.tolist()\n",
    "print(len(sustained_speeds),'trips have sustained speeds above',maxspeed,'mph')\n",
    "coords = coords[~coords['tripid'].isin(sustained_speeds)]\n",
    "print(coords['tripid'].nunique(),'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hAccuracy Filter\n",
    "It's not clear how much this metric helps, but it may help to remove points that are substantially above the mean hAccuracy level for that trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275100 points were 4 standard deviations above the mean hAccuracy\n"
     ]
    }
   ],
   "source": [
    "z_score = coords.groupby('tripid')['hAccuracy_ft'].apply(lambda group: (((group - group.mean())) / group.std()) > 4)\n",
    "print(z_score.sum(),'points were 4 standard deviations above the mean hAccuracy')\n",
    "coords = coords[~z_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store coords in dictionary and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_dict = {}\n",
    "coords_dict.update({tripid : df.reset_index(drop=True) for tripid, df in coords.groupby('tripid')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20430"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coords_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some speed values are NULL, but these trips appear to still be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO fix this\n",
    "# #any NA readings?\n",
    "# coords[coords['speed_mph'].isna()]\n",
    "# null_speeds = coords.groupby('tripid')['speed_mph'].apply(lambda speed: speed.isna().sum()).sort_values(ascending=False)\n",
    "# null_speeds\n",
    "# group_size = coords['tripid'].value_counts()\n",
    "# group_size.index.name = 'tripid'\n",
    "# test = pd.concat([null_speeds,group_size],axis=1,ignore_index=False)\n",
    "# test.sort_values('speed_mph',ascending=False).head(10)\n",
    "# coords[coords['tripid']==25308].drop(columns=['datetime']).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_count = [[key,item.shape[0]] for key, item in coords_dict.items()]\n",
    "df = pd.DataFrame(gps_count,columns=['tripid','points'])\n",
    "remove = df.loc[df['points'] < point_threshold,'tripid'].tolist()\n",
    "\n",
    "for tripid in remove:\n",
    "    coords_dict.pop(tripid)\n",
    "\n",
    "trips_df = trips_df[~trips_df['tripid'].isin(remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20359"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coords_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20359\n"
     ]
    }
   ],
   "source": [
    "print(trips_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure trips_df and coords_dict match up\n",
    "trips_df = trips_df[trips_df['tripid'].isin(list(coords_dict.keys()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caluclate new trip metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tripid, coords in coords_dict.items():\n",
    "    coords = gps_utils.calculate_coordinate_metrics(coords)\n",
    "    trips_df = gps_utils.calculate_trip_metrics(tripid,coords,trips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (export_fp/'trips_0.pkl').open('wb') as fh:\n",
    "    pickle.dump(trips_df,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (export_fp/'coords_0.pkl').open('wb') as fh:\n",
    "    pickle.dump(coords_dict,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the first 5 seconds of a trip (in-case the GPS location wasn't immediately triangulated)\n",
    "Use this format for more complicated cleaning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_first_seconds(group,seconds):\n",
    "#     start_time = group.min()\n",
    "#     after = (group - start_time) > datetime.timedelta(seconds=seconds)\n",
    "#     return group[after]\n",
    "# drop_first_seconds = coords.groupby('tripid')['datetime'].apply(lambda group: remove_first_seconds(group,5)).reset_index()['level_1']\n",
    "# before_drop = coords.shape[0]\n",
    "# coords = coords.loc[drop_first_seconds]\n",
    "# print(before_drop-coords.shape[0],'coordinates dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed deviation\n",
    "Takes 8 mins. Removed because it takes too long, and it doesn't really seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_list_speed_deviation = []\n",
    "# speed_deviation_max_mph = 47\n",
    "\n",
    "# for tripid, coords_df in tqdm(coords_dict.items()):\n",
    "#     '''\n",
    "#     This loop searches for unrealistic jumps between gps points\n",
    "#     and removes these points until there are no more jumps.\n",
    "\n",
    "#     It first looks at the first point in the data to see if there\n",
    "#     is a jump after it. This helps remove points that were recorded before\n",
    "#     the GPS had been triangulated. If the first two points are recorded close together,\n",
    "#     then this wouldn't help.\n",
    "    \n",
    "#     Then it looks at subsequent points and removes them if the threshold is\n",
    "#     exceeded.\n",
    "    \n",
    "#     All the while, trips are removed if this process eliminates too many points\n",
    "#     '''\n",
    "    \n",
    "#     # #calculate metrics for the coordinates dataframe\n",
    "#     # orig_cols = coords_df.columns\n",
    "#     # coords_df = gps_utils.calculate_coordinate_metrics(coords_df)\n",
    "    \n",
    "#     # if second value has speed above speed_deviation_max_mph remove the first point until no jump is dectected\n",
    "#     # this should deal with cases in which the first point is way off\n",
    "#     while coords_df.iloc[1]['speed_mph'] >= speed_deviation_max_mph:\n",
    "#         coords_df = coords_df.iloc[1:,:]\n",
    "#         #remove if this gets rid of too many points\n",
    "#         if coords_df.shape[0] < 3:\n",
    "#             #remove_list_speed_deviation.append(tripid)\n",
    "#             break\n",
    "#         # #recalculate metrics\n",
    "#         # coords_df = gps_utils.calculate_coordinate_metrics(coords_df)\n",
    "    \n",
    "#     if coords_df.shape[0] < point_threshold:\n",
    "#         remove_list_speed_deviation.append(tripid)\n",
    "#         continue\n",
    "\n",
    "#     # remove point if speed_deviation_max_mph between it and previous point\n",
    "#     while (coords_df['speed_mph']>=speed_deviation_max_mph).sum() > 0:\n",
    "#         # only keep if below speed_deviation_max_mph mph or it's the first row and has na as the value\n",
    "#         coords_df = coords_df[(coords_df['speed_mph']<speed_deviation_max_mph) | (coords_df['speed_mph'].isna())]\n",
    "#         # make sure there are enough points\n",
    "#         if coords_df.shape[0] < point_threshold:\n",
    "#             #emove_list_speed_deviation.append(tripid)\n",
    "#             break\n",
    "#         #recalculate metrics\n",
    "#         coords_df = gps_utils.calculate_coordinate_metrics(coords_df)\n",
    "\n",
    "#     if coords_df.shape[0] < point_threshold:\n",
    "#         remove_list_speed_deviation.append(tripid)\n",
    "#         continue\n",
    "\n",
    "#     # remove extra columns\n",
    "#     coords_df = coords_df[orig_cols]\n",
    "    \n",
    "#     # assign cleaned coords to dictionary\n",
    "#     coords_dict[tripid] = coords_df\n",
    "\n",
    "# for key in remove_list_speed_deviation:\n",
    "#     coords_dict.pop(key)\n",
    "# trips_df = trips_df[~trips_df['tripid'].isin(remove_list_speed_deviation)]\n",
    "# print(len(remove_list_speed_deviation),'trips removed after speed deviation filter')\n",
    "# print(trips_df.shape[0],'trips remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim out trip chains\n",
    "Identifies GPS trip chains and trim trips.\n",
    "\n",
    "Some trips will have gaps between point recordings. When the gap is sufficiently large, it\n",
    "indicates that the person may have forgotten to stop their recording,\n",
    "stopped at a destination along the way (trip chaining), or that there was some other error.\n",
    "\n",
    "This algorithm checks for cases in which the person probably forgot to stop\n",
    "the recording and trims the excess points.\n",
    "\n",
    "If a chain was detected, then only the first leg of the trip is retained.\n",
    "There are only a small number of these.\n",
    "\n",
    "Step 0: Set the pause threshold\n",
    "Step 1: For each trip, count the number of points that exceed this threshold\n",
    "Step 2: For the first pause detected, trim trip if all subsequent points are\n",
    "within pause threshold biking distance (using avg speed of 8 mph)\n",
    "Step 3: If not all points are contained, then consider these points to be part of a new trip\n",
    "chain. Repeat step 1 until all legs of trip chains are identfied.\n",
    "Step 4: Remove trip chains from databases and export them for later examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_list_tripchain = []\n",
    "# pause_threshold_min = 5\n",
    "\n",
    "# for tripid, coords_df in tqdm(coords_dict.items()):\n",
    "#     '''\n",
    "#     Identifies GPS trip chains and trim trips\n",
    "\n",
    "#     Some trips will have gaps between point recordings. When the gap is large, it\n",
    "#     indicates that the person may have forgotten to stop their recording,\n",
    "#     stopped at a destination along the way (trip chaining), or that there was some error.\n",
    "\n",
    "#     This algorithm checks for cases in which the person probably forgot to stop\n",
    "#     the recording and trims the excess points.\n",
    "\n",
    "#     If a chain was detected, then only the first leg of the trip is retained.\n",
    "#     There are only a small number of these.\n",
    "\n",
    "#     Step 0: Set the pause threshold\n",
    "#     Step 1: For each trip, count the number of points that exceed this threshold\n",
    "#     Step 2: For the first pause detected, trim trip if all subsequent points are\n",
    "#     within pause threshold biking distance (using avg speed of 8 mph)\n",
    "#     Step 3: If not all points are contained, then consider these points to be part of a new trip\n",
    "#     chain. Repeat step 1 until all legs of trip chains are identfied.\n",
    "#     Step 4: Remove trip chains from databases and export them for later examination.\n",
    "\n",
    "#     '''\n",
    "#     #calculate metrics for the coordinates dataframe\n",
    "#     orig_cols = coords_df.columns\n",
    "#     coords_df = gps_utils.calculate_coordinate_metrics(coords_df)\n",
    "\n",
    "\n",
    "    \n",
    "#     # reset index to ensure subsequent numbers\n",
    "#     coords_df.reset_index(drop=True,inplace=True)    \n",
    "\n",
    "#     #TODO split the trip into multiple segments and assign it a new tripid\n",
    "#     #while (coords_df['delta_time'] > datetime.timedelta(minutes=pause_threshold_min)).any():\n",
    "#     if (coords_df['delta_time'] > datetime.timedelta(minutes=pause_threshold_min)).any():\n",
    "        \n",
    "#         #find first pause\n",
    "#         first_pause = (coords_df['delta_time'] > datetime.timedelta(minutes=pause_threshold_min)).idxmax()\n",
    "        \n",
    "#         #trim trip\n",
    "#         coords_df = coords_df.loc[0:first_pause]        \n",
    "\n",
    "#     coords_df = gps_utils.calculate_coordinate_metrics(coords_df)\n",
    "\n",
    "#     if coords_df.shape[0] < point_threshold:\n",
    "#         remove_list_tripchain.append(tripid)\n",
    "#         continue\n",
    "\n",
    "#     # remove extra columns\n",
    "#     coords_df = coords_df[orig_cols]\n",
    "#     # assign cleaned coords to dictionary\n",
    "#     coords_dict[tripid] = coords_df\n",
    "# d\n",
    "# for key in remove_list_tripchain:\n",
    "#     coords_dict.pop(key)\n",
    "# trips_df = trips_df[~trips_df['tripid'].isin(remove_list_tripchain)]\n",
    "# print(len(remove_list_tripchain),'trips removed during trip chain detection')\n",
    "# print(trips_df.shape[0],'trips remaining')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
